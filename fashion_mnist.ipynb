{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wJImnLRvS10E"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zachhom/170-P3-Neural-Net/blob/Haneul/fashion_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writeup\n",
        "\n",
        "For the writeup, please include the following information:\n",
        "\n",
        "\n",
        "*   Describe the architecture of your neural network\n",
        "*   Discuss how you tuned your network and why you think it's performance is reasonable for this task\n",
        "*   Discuss whether or not you feel that this classifier is appropriate for the given task (check PA3 description)\n",
        "\n"
      ],
      "metadata": {
        "id": "qxOhrr4P1jWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writeup\n",
        "\n",
        "### `Describe the architecture of your neural network`\n",
        "\n",
        "Our architecture uses the sequentially-layered architecture that adds the (Convolution, Max Pooling) as a C-MP combination and Dense layer after two loops of layer propagation from C-MP layers, we further add Dense Layer to the model before the output layer.\n",
        "\n",
        "### `Discuss how you tuned your network and why you think it's performance is reasonable for this task`\n",
        "\n",
        "The overall idea is to `increase accuracy value` while `decresing value loss`. Based on the overall trend while epochs being run, we attempted to tune the basic primitives constants like `Epoch` and `Batch Size Train`.\n",
        "\n",
        "From our understanding, we have the best result when we operate on epoch/batch_train_size. ratio of nearly 1/100 or 0.01. From our understanding, each epoch will train 100 units from the training dataset. We thought that this makes sence because we have 10 categories to disclose the output type. Thus we thought we would be training every category at least 5 times in an epoch loop. \n",
        "\n",
        "We get overall trend of value accuracy about 92 % and the value lost around at 0.17~0.20. This was the best overall set of data values among these categories.\n",
        "\n",
        "\n",
        "### `Discuss whether or not you feel that this classifier is appropriate for the given task (check PA3 description)`\n",
        "\n",
        "The performance for our network was 92%. Given a large dataset such as fashionMNIST, an 8% failure rate is a reasonable classifier because that means we have a relatively high success rate. Given that this is a clothing identification network, we can accept an error rate such as 8%. However, if the task was different such as facial recognition for a counter terrorism agency, 8% failure rate would be too high.\n"
      ],
      "metadata": {
        "id": "eVQezXswuZmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow"
      ],
      "metadata": {
        "id": "4XcY-TlhGxru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Imports"
      ],
      "metadata": {
        "id": "KyU1ThEPG0Mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout"
      ],
      "metadata": {
        "id": "0F1QY_IOG2e1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Set Hyperparameters"
      ],
      "metadata": {
        "id": "qJe1xk4QHEvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tune these\n",
        "\n",
        "n_epochs = 125\n",
        "batch_size_train = 4500\n",
        "batch_size_test = 10000\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "tsXzcHqDHTku"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Data"
      ],
      "metadata": {
        "id": "n13sUlF_HJ1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'fashion_mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "ds_train = ds_train.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(batch_size_train)\n",
        "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_test = ds_test.batch(batch_size_test)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "W4uIM9TaHWUH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define Model"
      ],
      "metadata": {
        "id": "DgEqnLBsR3W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Convolutional Layers\n",
        "# model.add(tf.keras.layers.Conv2D(20, (2,2), padding='same', activation='relu', input_shape=(28,28,1)))\n",
        "# model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "# model.add(tf.keras.layers.Conv2D(40, (2,2), padding='same', activation='relu'))\n",
        "# model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3)) \n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Dense Layers\n",
        "# model.add(tf.keras.layers.Flatten())\n",
        "# model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "# model.add(tf.keras.layers.Dense(16, activation='relu')) #256\n",
        "# model.add(tf.keras.layers.Dense(126, activation='relu'))\n",
        "# model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "v4wX6lQOHjeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8012cf3f-f55a-4856-855a-b98e2dc75f8d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 64)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 32)        8224      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 7, 7, 32)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1568)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               401664    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 427,306\n",
            "Trainable params: 427,306\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things to consider:\n",
        "\n",
        "* dropout, convensation, things like that\n",
        "* overtrain\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "es5iz2n7jGpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LRVf9D0mjGel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train Model"
      ],
      "metadata": {
        "id": "f5VZLjUSHNl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    ds_train,\n",
        "    epochs=n_epochs,\n",
        "    validation_data=ds_test,\n",
        ")\n"
      ],
      "metadata": {
        "id": "GQp_7VQtHwXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f466ad-6f4d-4e48-d672-d4359f8fc7dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 11s 259ms/step - loss: 1.5492 - sparse_categorical_accuracy: 0.4287 - val_loss: 0.7093 - val_sparse_categorical_accuracy: 0.7339\n",
            "Epoch 2/125\n",
            "14/14 [==============================] - 2s 132ms/step - loss: 0.7058 - sparse_categorical_accuracy: 0.7337 - val_loss: 0.5357 - val_sparse_categorical_accuracy: 0.7877\n",
            "Epoch 3/125\n",
            "14/14 [==============================] - 2s 131ms/step - loss: 0.5606 - sparse_categorical_accuracy: 0.7872 - val_loss: 0.4589 - val_sparse_categorical_accuracy: 0.8225\n",
            "Epoch 4/125\n",
            "14/14 [==============================] - 2s 140ms/step - loss: 0.4971 - sparse_categorical_accuracy: 0.8135 - val_loss: 0.4154 - val_sparse_categorical_accuracy: 0.8445\n",
            "Epoch 5/125\n",
            "14/14 [==============================] - 2s 132ms/step - loss: 0.4552 - sparse_categorical_accuracy: 0.8321 - val_loss: 0.3779 - val_sparse_categorical_accuracy: 0.8563\n",
            "Epoch 6/125\n",
            "14/14 [==============================] - 2s 131ms/step - loss: 0.4177 - sparse_categorical_accuracy: 0.8498 - val_loss: 0.3497 - val_sparse_categorical_accuracy: 0.8701\n",
            "Epoch 7/125\n",
            "14/14 [==============================] - 2s 130ms/step - loss: 0.3950 - sparse_categorical_accuracy: 0.8562 - val_loss: 0.3381 - val_sparse_categorical_accuracy: 0.8765\n",
            "Epoch 8/125\n",
            "14/14 [==============================] - 2s 131ms/step - loss: 0.3786 - sparse_categorical_accuracy: 0.8629 - val_loss: 0.3366 - val_sparse_categorical_accuracy: 0.8783\n",
            "Epoch 9/125\n",
            "14/14 [==============================] - 2s 138ms/step - loss: 0.3619 - sparse_categorical_accuracy: 0.8692 - val_loss: 0.3224 - val_sparse_categorical_accuracy: 0.8814\n",
            "Epoch 10/125\n",
            "14/14 [==============================] - 2s 133ms/step - loss: 0.3269 - sparse_categorical_accuracy: 0.8824 - val_loss: 0.2846 - val_sparse_categorical_accuracy: 0.8970\n",
            "Epoch 14/125\n",
            "14/14 [==============================] - 2s 132ms/step - loss: 0.3146 - sparse_categorical_accuracy: 0.8863 - val_loss: 0.2796 - val_sparse_categorical_accuracy: 0.9000\n",
            "Epoch 15/125\n",
            "14/14 [==============================] - 2s 131ms/step - loss: 0.3161 - sparse_categorical_accuracy: 0.8837 - val_loss: 0.2750 - val_sparse_categorical_accuracy: 0.8997\n",
            "Epoch 16/125\n",
            "14/14 [==============================] - 2s 133ms/step - loss: 0.3027 - sparse_categorical_accuracy: 0.8903 - val_loss: 0.2685 - val_sparse_categorical_accuracy: 0.8999\n",
            "Epoch 17/125\n",
            "14/14 [==============================] - 2s 133ms/step - loss: 0.2971 - sparse_categorical_accuracy: 0.8916 - val_loss: 0.2658 - val_sparse_categorical_accuracy: 0.9044\n",
            "Epoch 18/125\n",
            "14/14 [==============================] - 2s 133ms/step - loss: 0.2882 - sparse_categorical_accuracy: 0.8944 - val_loss: 0.2555 - val_sparse_categorical_accuracy: 0.9060\n",
            "Epoch 19/125\n",
            "14/14 [==============================] - 2s 133ms/step - loss: 0.2853 - sparse_categorical_accuracy: 0.8967 - val_loss: 0.2527 - val_sparse_categorical_accuracy: 0.9076\n",
            "Epoch 20/125\n",
            "14/14 [==============================] - 2s 132ms/step - loss: 0.3046 - sparse_categorical_accuracy: 0.8885 - val_loss: 0.2661 - val_sparse_categorical_accuracy: 0.9042\n",
            "Epoch 21/125\n",
            "14/14 [==============================] - 2s 133ms/step - loss: 0.2838 - sparse_categorical_accuracy: 0.8973 - val_loss: 0.2560 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 22/125\n",
            "14/14 [==============================] - 2s 134ms/step - loss: 0.2812 - sparse_categorical_accuracy: 0.8967 - val_loss: 0.2584 - val_sparse_categorical_accuracy: 0.9075\n",
            "Epoch 23/125\n",
            "14/14 [==============================] - 2s 133ms/step - loss: 0.2744 - sparse_categorical_accuracy: 0.9000 - val_loss: 0.2557 - val_sparse_categorical_accuracy: 0.9055\n",
            "Epoch 24/125\n",
            "14/14 [==============================] - 2s 134ms/step - loss: 0.2774 - sparse_categorical_accuracy: 0.8985 - val_loss: 0.2488 - val_sparse_categorical_accuracy: 0.9106\n",
            "Epoch 25/125\n",
            "14/14 [==============================] - 2s 134ms/step - loss: 0.2651 - sparse_categorical_accuracy: 0.9036 - val_loss: 0.2417 - val_sparse_categorical_accuracy: 0.9104\n",
            "Epoch 26/125\n",
            "14/14 [==============================] - 2s 133ms/step - loss: 0.2674 - sparse_categorical_accuracy: 0.9029 - val_loss: 0.2396 - val_sparse_categorical_accuracy: 0.9119\n",
            "Epoch 27/125\n",
            "14/14 [==============================] - 2s 133ms/step - loss: 0.2635 - sparse_categorical_accuracy: 0.9032 - val_loss: 0.2414 - val_sparse_categorical_accuracy: 0.9109\n",
            "Epoch 28/125\n",
            "14/14 [==============================] - 2s 133ms/step - loss: 0.2600 - sparse_categorical_accuracy: 0.9032 - val_loss: 0.2382 - val_sparse_categorical_accuracy: 0.9122\n",
            "Epoch 29/125\n",
            "14/14 [==============================] - 2s 134ms/step - loss: 0.2568 - sparse_categorical_accuracy: 0.9046 - val_loss: 0.2409 - val_sparse_categorical_accuracy: 0.9128\n",
            "Epoch 30/125\n",
            "14/14 [==============================] - 2s 134ms/step - loss: 0.2681 - sparse_categorical_accuracy: 0.9025 - val_loss: 0.2508 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 31/125\n",
            "14/14 [==============================] - 2s 134ms/step - loss: 0.2650 - sparse_categorical_accuracy: 0.9019 - val_loss: 0.2508 - val_sparse_categorical_accuracy: 0.9092\n",
            "Epoch 32/125\n",
            "14/14 [==============================] - 2s 134ms/step - loss: 0.2522 - sparse_categorical_accuracy: 0.9077 - val_loss: 0.2337 - val_sparse_categorical_accuracy: 0.9143\n",
            "Epoch 33/125\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.2494 - sparse_categorical_accuracy: 0.9089 - val_loss: 0.2332 - val_sparse_categorical_accuracy: 0.9153\n",
            "Epoch 34/125\n",
            "14/14 [==============================] - 2s 134ms/step - loss: 0.2441 - sparse_categorical_accuracy: 0.9109 - val_loss: 0.2295 - val_sparse_categorical_accuracy: 0.9162\n",
            "Epoch 35/125\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.2436 - sparse_categorical_accuracy: 0.9108 - val_loss: 0.2305 - val_sparse_categorical_accuracy: 0.9156\n",
            "Epoch 36/125\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.2448 - sparse_categorical_accuracy: 0.9094 - val_loss: 0.2313 - val_sparse_categorical_accuracy: 0.9188\n",
            "Epoch 37/125\n",
            "14/14 [==============================] - 2s 148ms/step - loss: 0.2395 - sparse_categorical_accuracy: 0.9132 - val_loss: 0.2273 - val_sparse_categorical_accuracy: 0.9153\n",
            "Epoch 38/125\n",
            "14/14 [==============================] - 2s 138ms/step - loss: 0.2379 - sparse_categorical_accuracy: 0.9121 - val_loss: 0.2293 - val_sparse_categorical_accuracy: 0.9173\n",
            "Epoch 39/125\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.2362 - sparse_categorical_accuracy: 0.9126 - val_loss: 0.2283 - val_sparse_categorical_accuracy: 0.9156\n",
            "Epoch 40/125\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.2368 - sparse_categorical_accuracy: 0.9122 - val_loss: 0.2316 - val_sparse_categorical_accuracy: 0.9173\n",
            "Epoch 41/125\n",
            "14/14 [==============================] - 2s 139ms/step - loss: 0.2351 - sparse_categorical_accuracy: 0.9135 - val_loss: 0.2311 - val_sparse_categorical_accuracy: 0.9147\n",
            "Epoch 42/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2367 - sparse_categorical_accuracy: 0.9130 - val_loss: 0.2275 - val_sparse_categorical_accuracy: 0.9192\n",
            "Epoch 43/125\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.2336 - sparse_categorical_accuracy: 0.9147 - val_loss: 0.2234 - val_sparse_categorical_accuracy: 0.9188\n",
            "Epoch 44/125\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.2283 - sparse_categorical_accuracy: 0.9152 - val_loss: 0.2303 - val_sparse_categorical_accuracy: 0.9159\n",
            "Epoch 45/125\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.2316 - sparse_categorical_accuracy: 0.9132 - val_loss: 0.2290 - val_sparse_categorical_accuracy: 0.9167\n",
            "Epoch 46/125\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.2289 - sparse_categorical_accuracy: 0.9158 - val_loss: 0.2242 - val_sparse_categorical_accuracy: 0.9175\n",
            "Epoch 47/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2290 - sparse_categorical_accuracy: 0.9157 - val_loss: 0.2305 - val_sparse_categorical_accuracy: 0.9172\n",
            "Epoch 48/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2254 - sparse_categorical_accuracy: 0.9169 - val_loss: 0.2262 - val_sparse_categorical_accuracy: 0.9175\n",
            "Epoch 49/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2281 - sparse_categorical_accuracy: 0.9169 - val_loss: 0.2377 - val_sparse_categorical_accuracy: 0.9132\n",
            "Epoch 50/125\n",
            "14/14 [==============================] - 2s 140ms/step - loss: 0.2262 - sparse_categorical_accuracy: 0.9161 - val_loss: 0.2289 - val_sparse_categorical_accuracy: 0.9151\n",
            "Epoch 51/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2217 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.2227 - val_sparse_categorical_accuracy: 0.9185\n",
            "Epoch 52/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.2175 - sparse_categorical_accuracy: 0.9177 - val_loss: 0.2181 - val_sparse_categorical_accuracy: 0.9215\n",
            "Epoch 53/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.2204 - sparse_categorical_accuracy: 0.9191 - val_loss: 0.2262 - val_sparse_categorical_accuracy: 0.9188\n",
            "Epoch 54/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.2226 - sparse_categorical_accuracy: 0.9168 - val_loss: 0.2296 - val_sparse_categorical_accuracy: 0.9173\n",
            "Epoch 55/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.2207 - sparse_categorical_accuracy: 0.9186 - val_loss: 0.2288 - val_sparse_categorical_accuracy: 0.9169\n",
            "Epoch 56/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.2178 - sparse_categorical_accuracy: 0.9197 - val_loss: 0.2256 - val_sparse_categorical_accuracy: 0.9179\n",
            "Epoch 57/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.2150 - sparse_categorical_accuracy: 0.9195 - val_loss: 0.2222 - val_sparse_categorical_accuracy: 0.9202\n",
            "Epoch 58/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.2100 - sparse_categorical_accuracy: 0.9227 - val_loss: 0.2235 - val_sparse_categorical_accuracy: 0.9202\n",
            "Epoch 59/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.2167 - sparse_categorical_accuracy: 0.9206 - val_loss: 0.2297 - val_sparse_categorical_accuracy: 0.9159\n",
            "Epoch 60/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.2167 - sparse_categorical_accuracy: 0.9199 - val_loss: 0.2331 - val_sparse_categorical_accuracy: 0.9142\n",
            "Epoch 61/125\n",
            "14/14 [==============================] - 2s 139ms/step - loss: 0.2169 - sparse_categorical_accuracy: 0.9205 - val_loss: 0.2282 - val_sparse_categorical_accuracy: 0.9176\n",
            "Epoch 62/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2136 - sparse_categorical_accuracy: 0.9218 - val_loss: 0.2231 - val_sparse_categorical_accuracy: 0.9211\n",
            "Epoch 63/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2084 - sparse_categorical_accuracy: 0.9229 - val_loss: 0.2219 - val_sparse_categorical_accuracy: 0.9206\n",
            "Epoch 64/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2134 - sparse_categorical_accuracy: 0.9215 - val_loss: 0.2309 - val_sparse_categorical_accuracy: 0.9150\n",
            "Epoch 65/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2125 - sparse_categorical_accuracy: 0.9214 - val_loss: 0.2253 - val_sparse_categorical_accuracy: 0.9193\n",
            "Epoch 66/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2041 - sparse_categorical_accuracy: 0.9245 - val_loss: 0.2215 - val_sparse_categorical_accuracy: 0.9192\n",
            "Epoch 67/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2127 - sparse_categorical_accuracy: 0.9212 - val_loss: 0.2226 - val_sparse_categorical_accuracy: 0.9198\n",
            "Epoch 68/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2058 - sparse_categorical_accuracy: 0.9249 - val_loss: 0.2301 - val_sparse_categorical_accuracy: 0.9151\n",
            "Epoch 69/125\n",
            "14/14 [==============================] - 2s 138ms/step - loss: 0.2107 - sparse_categorical_accuracy: 0.9212 - val_loss: 0.2305 - val_sparse_categorical_accuracy: 0.9195\n",
            "Epoch 70/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2145 - sparse_categorical_accuracy: 0.9202 - val_loss: 0.2299 - val_sparse_categorical_accuracy: 0.9172\n",
            "Epoch 71/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2081 - sparse_categorical_accuracy: 0.9227 - val_loss: 0.2292 - val_sparse_categorical_accuracy: 0.9211\n",
            "Epoch 72/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.2078 - sparse_categorical_accuracy: 0.9228 - val_loss: 0.2278 - val_sparse_categorical_accuracy: 0.9198\n",
            "Epoch 73/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2044 - sparse_categorical_accuracy: 0.9239 - val_loss: 0.2204 - val_sparse_categorical_accuracy: 0.9233\n",
            "Epoch 74/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.2051 - sparse_categorical_accuracy: 0.9254 - val_loss: 0.2270 - val_sparse_categorical_accuracy: 0.9196\n",
            "Epoch 75/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2028 - sparse_categorical_accuracy: 0.9236 - val_loss: 0.2251 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 76/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2020 - sparse_categorical_accuracy: 0.9252 - val_loss: 0.2228 - val_sparse_categorical_accuracy: 0.9219\n",
            "Epoch 77/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2000 - sparse_categorical_accuracy: 0.9265 - val_loss: 0.2193 - val_sparse_categorical_accuracy: 0.9222\n",
            "Epoch 78/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1941 - sparse_categorical_accuracy: 0.9292 - val_loss: 0.2131 - val_sparse_categorical_accuracy: 0.9259\n",
            "Epoch 79/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1997 - sparse_categorical_accuracy: 0.9268 - val_loss: 0.2254 - val_sparse_categorical_accuracy: 0.9207\n",
            "Epoch 80/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.1991 - sparse_categorical_accuracy: 0.9267 - val_loss: 0.2222 - val_sparse_categorical_accuracy: 0.9214\n",
            "Epoch 81/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1984 - sparse_categorical_accuracy: 0.9258 - val_loss: 0.2289 - val_sparse_categorical_accuracy: 0.9204\n",
            "Epoch 82/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1996 - sparse_categorical_accuracy: 0.9262 - val_loss: 0.2289 - val_sparse_categorical_accuracy: 0.9209\n",
            "Epoch 83/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.2032 - sparse_categorical_accuracy: 0.9247 - val_loss: 0.2187 - val_sparse_categorical_accuracy: 0.9228\n",
            "Epoch 84/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1954 - sparse_categorical_accuracy: 0.9273 - val_loss: 0.2246 - val_sparse_categorical_accuracy: 0.9219\n",
            "Epoch 85/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1966 - sparse_categorical_accuracy: 0.9288 - val_loss: 0.2191 - val_sparse_categorical_accuracy: 0.9221\n",
            "Epoch 86/125\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.1935 - sparse_categorical_accuracy: 0.9279 - val_loss: 0.2192 - val_sparse_categorical_accuracy: 0.9227\n",
            "Epoch 87/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1939 - sparse_categorical_accuracy: 0.9274 - val_loss: 0.2196 - val_sparse_categorical_accuracy: 0.9250\n",
            "Epoch 88/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1952 - sparse_categorical_accuracy: 0.9274 - val_loss: 0.2203 - val_sparse_categorical_accuracy: 0.9225\n",
            "Epoch 89/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1925 - sparse_categorical_accuracy: 0.9284 - val_loss: 0.2216 - val_sparse_categorical_accuracy: 0.9235\n",
            "Epoch 90/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1914 - sparse_categorical_accuracy: 0.9291 - val_loss: 0.2177 - val_sparse_categorical_accuracy: 0.9233\n",
            "Epoch 91/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.2019 - sparse_categorical_accuracy: 0.9260 - val_loss: 0.2349 - val_sparse_categorical_accuracy: 0.9157\n",
            "Epoch 92/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.1965 - sparse_categorical_accuracy: 0.9271 - val_loss: 0.2219 - val_sparse_categorical_accuracy: 0.9225\n",
            "Epoch 93/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1934 - sparse_categorical_accuracy: 0.9291 - val_loss: 0.2254 - val_sparse_categorical_accuracy: 0.9220\n",
            "Epoch 94/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1894 - sparse_categorical_accuracy: 0.9300 - val_loss: 0.2276 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 95/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.1898 - sparse_categorical_accuracy: 0.9298 - val_loss: 0.2200 - val_sparse_categorical_accuracy: 0.9233\n",
            "Epoch 96/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1856 - sparse_categorical_accuracy: 0.9311 - val_loss: 0.2237 - val_sparse_categorical_accuracy: 0.9239\n",
            "Epoch 97/125\n",
            "14/14 [==============================] - 2s 140ms/step - loss: 0.1907 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.2292 - val_sparse_categorical_accuracy: 0.9199\n",
            "Epoch 98/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1951 - sparse_categorical_accuracy: 0.9284 - val_loss: 0.2171 - val_sparse_categorical_accuracy: 0.9241\n",
            "Epoch 99/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.1890 - sparse_categorical_accuracy: 0.9313 - val_loss: 0.2285 - val_sparse_categorical_accuracy: 0.9218\n",
            "Epoch 100/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1905 - sparse_categorical_accuracy: 0.9298 - val_loss: 0.2179 - val_sparse_categorical_accuracy: 0.9254\n",
            "Epoch 101/125\n",
            "14/14 [==============================] - 2s 140ms/step - loss: 0.1870 - sparse_categorical_accuracy: 0.9306 - val_loss: 0.2219 - val_sparse_categorical_accuracy: 0.9222\n",
            "Epoch 102/125\n",
            "14/14 [==============================] - 2s 139ms/step - loss: 0.1852 - sparse_categorical_accuracy: 0.9301 - val_loss: 0.2246 - val_sparse_categorical_accuracy: 0.9211\n",
            "Epoch 103/125\n",
            "14/14 [==============================] - 2s 143ms/step - loss: 0.1910 - sparse_categorical_accuracy: 0.9297 - val_loss: 0.2198 - val_sparse_categorical_accuracy: 0.9222\n",
            "Epoch 104/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1830 - sparse_categorical_accuracy: 0.9322 - val_loss: 0.2259 - val_sparse_categorical_accuracy: 0.9222\n",
            "Epoch 105/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1862 - sparse_categorical_accuracy: 0.9314 - val_loss: 0.2276 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 106/125\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.1887 - sparse_categorical_accuracy: 0.9301 - val_loss: 0.2229 - val_sparse_categorical_accuracy: 0.9237\n",
            "Epoch 107/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1868 - sparse_categorical_accuracy: 0.9316 - val_loss: 0.2224 - val_sparse_categorical_accuracy: 0.9245\n",
            "Epoch 108/125\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.1852 - sparse_categorical_accuracy: 0.9323 - val_loss: 0.2182 - val_sparse_categorical_accuracy: 0.9245\n",
            "Epoch 109/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.1830 - sparse_categorical_accuracy: 0.9327 - val_loss: 0.2196 - val_sparse_categorical_accuracy: 0.9268\n",
            "Epoch 110/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.1835 - sparse_categorical_accuracy: 0.9320 - val_loss: 0.2222 - val_sparse_categorical_accuracy: 0.9247\n",
            "Epoch 111/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.1803 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.2226 - val_sparse_categorical_accuracy: 0.9242\n",
            "Epoch 112/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1829 - sparse_categorical_accuracy: 0.9330 - val_loss: 0.2189 - val_sparse_categorical_accuracy: 0.9242\n",
            "Epoch 113/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.1895 - sparse_categorical_accuracy: 0.9290 - val_loss: 0.2207 - val_sparse_categorical_accuracy: 0.9223\n",
            "Epoch 114/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1805 - sparse_categorical_accuracy: 0.9341 - val_loss: 0.2290 - val_sparse_categorical_accuracy: 0.9196\n",
            "Epoch 115/125\n",
            "14/14 [==============================] - 2s 138ms/step - loss: 0.1823 - sparse_categorical_accuracy: 0.9320 - val_loss: 0.2152 - val_sparse_categorical_accuracy: 0.9258\n",
            "Epoch 116/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1795 - sparse_categorical_accuracy: 0.9347 - val_loss: 0.2210 - val_sparse_categorical_accuracy: 0.9227\n",
            "Epoch 117/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1788 - sparse_categorical_accuracy: 0.9341 - val_loss: 0.2234 - val_sparse_categorical_accuracy: 0.9235\n",
            "Epoch 118/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1801 - sparse_categorical_accuracy: 0.9340 - val_loss: 0.2208 - val_sparse_categorical_accuracy: 0.9234\n",
            "Epoch 119/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.1821 - sparse_categorical_accuracy: 0.9332 - val_loss: 0.2215 - val_sparse_categorical_accuracy: 0.9248\n",
            "Epoch 120/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.1796 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.2226 - val_sparse_categorical_accuracy: 0.9252\n",
            "Epoch 121/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.1800 - sparse_categorical_accuracy: 0.9339 - val_loss: 0.2323 - val_sparse_categorical_accuracy: 0.9186\n",
            "Epoch 122/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1835 - sparse_categorical_accuracy: 0.9319 - val_loss: 0.2193 - val_sparse_categorical_accuracy: 0.9267\n",
            "Epoch 123/125\n",
            "14/14 [==============================] - 2s 135ms/step - loss: 0.1775 - sparse_categorical_accuracy: 0.9330 - val_loss: 0.2241 - val_sparse_categorical_accuracy: 0.9232\n",
            "Epoch 124/125\n",
            "14/14 [==============================] - 2s 136ms/step - loss: 0.1795 - sparse_categorical_accuracy: 0.9343 - val_loss: 0.2201 - val_sparse_categorical_accuracy: 0.9250\n",
            "Epoch 125/125\n",
            "14/14 [==============================] - 2s 137ms/step - loss: 0.1802 - sparse_categorical_accuracy: 0.9331 - val_loss: 0.2262 - val_sparse_categorical_accuracy: 0.9254\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbb40077d50>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}